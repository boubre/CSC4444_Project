\documentclass[mathserif]{beamer}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bm}
\usetheme{Copenhagen}

\begin{document}
	\title[Analyzing Stochastic Time-Series Data]{Techniques for Analyzing Stochastic Time-Series Data}
	\author[Castleberry \and Oubre \and Yu]{Dennis Castleberry \and Brandon Oubre \and Haikou Yu}
	\date{\today}
	\frame{\titlepage}
	
	\section{Naive Bayes}
	\subsection{Overview}
	\begin{frame}
		\frametitle{The Naive Bayes Classifier}
		\begin{itemize}
			\item Reduce classification to probability. What is \(P(class | attribute1, attribute2, ..., attributeN)\).
			\item Assumes that each attribute is independent of the others. (Hence the ``Naive'' nickname.)
			\item For example, let's consider if a car is stolen using \(P(stolen | Color, Type)\). Naive Bayes will assume \(color=red\) and \(type=sportscar\) to be independent.
			\item Naive Bayes is not sensitive to irrelevant attributes, since the probabilities of such attributes will be similar for all classes.
			\item Naive Bayes is quick to train, as it requires only one pass-though of the training data.
		\end{itemize}
	\end{frame}
	
	\subsection{Example}
	\begin{frame}
		\frametitle{Naive Bayes in Action}
		\begin{center} 
			\textbf{Training Data} \\
			\begin{tabular}{c | c | c || c}
				\textbf{Over 170cm} & \textbf{Eye Color} & \textbf{Hair Length} & \(\boxed{\textbf{Sex}}\) \\ \hline \hline
				No & Blue & Short & Male \\ \hline
				Yes & Brown & Long & Female \\ \hline
				No & Blue & Long & Female \\ \hline
				Yes & Brown & Short & Male \\ \hline
				Yes & Brown & Short & Female \\
			\end{tabular}  \\
			\small{Only discrete values shown, but we can still interpret real data using normal distributions!}
		\end{center}
			Suppose we are given an unseen data point \(\langle No, Blue, Short \rangle\). What should we classify it as?
	\end{frame}
	
	\begin{frame}
		\frametitle{Naive Bayes in Action}
		\begin{tabular}{l}
		\(P(Male|No,Blue,Short)\) \\
		\(=\dfrac{P(No,Blue,Short|Male) P(Male)}{P(No,Blue,Short)}\) \\
		\(= \alpha P(Male)\bm{P(No|Male)P(Blue|Male)P(Short|Male)}\) \\
		\(=\alpha \times \frac{2}{5} \times \frac{1}{2} \times \frac{1}{2} \times \frac{2}{2} = \boxed{0.1\alpha}\) \\ \\
		\hline \\
		\(P(Female|No,Blue,Short)\) \\
		\(=\alpha P(Female)P(No|Female)P(Blue|Female)P(Short|Female)\) \\
		\(= \alpha \times \frac{3}{5} \times \frac{1}{3} \times \frac{1}{3} \times \frac{1}{3} = \boxed{0.0\overline{2}\alpha}\) \\
		\end{tabular} \\
		\vspace{10px}
		Since \(P(Male|Data) > P(Female|Data)\), we classify the unseen point as Male. For multiple classes, just select the class with the greatest probability!
	\end{frame} 
	
	\section{Support Vector Machine}
	\begin{frame}
		\frametitle{Support Vector Machines (SVM)}
	\end{frame}
	
	\section{Neural Network}
	\begin{frame}
		\frametitle{Neural Networks}
	\end{frame}
	
\end{document}